{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXY-5-P0NteY",
        "outputId": "3eaa37d8-bbee-4671-efb9-ac01d375177f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbBkZBMN-V8P",
        "outputId": "af8e70d1-59cb-4632-f211-0adf5bf9d842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Drive successfully mounted!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Check if drive is mounted\n",
        "if 'drive' in locals() or 'drive' in globals():\n",
        "    print(\"Drive successfully mounted!\")\n",
        "else:\n",
        "    print(\"Drive not mounted.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgVhUJsv76GK",
        "outputId": "6750ce53-b163-4553-dec3-1cef974a4be0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.32.1-py3-none-any.whl (7.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.15.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.3 tokenizers-0.13.3 transformers-4.32.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.27.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HNba9OlNuiSZ",
        "outputId": "4fb3ff3b-ef77-4882-e457-796828159412"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25 - Loss: 0.7020\n",
            "Epoch 2/25 - Loss: 0.6809\n",
            "Epoch 3/25 - Loss: 0.6455\n",
            "Epoch 4/25 - Loss: 0.6049\n",
            "Epoch 5/25 - Loss: 0.5430\n",
            "Epoch 6/25 - Loss: 0.5185\n",
            "Epoch 7/25 - Loss: 0.4813\n",
            "Epoch 8/25 - Loss: 0.4512\n",
            "Epoch 9/25 - Loss: 0.4236\n",
            "Epoch 10/25 - Loss: 0.3792\n",
            "Epoch 11/25 - Loss: 0.3586\n",
            "Epoch 12/25 - Loss: 0.3326\n",
            "Epoch 13/25 - Loss: 0.3751\n",
            "Epoch 14/25 - Loss: 0.3086\n",
            "Epoch 15/25 - Loss: 0.3175\n",
            "Epoch 16/25 - Loss: 0.2138\n",
            "Epoch 17/25 - Loss: 0.2318\n",
            "Epoch 18/25 - Loss: 0.2258\n",
            "Epoch 19/25 - Loss: 0.1994\n",
            "Epoch 20/25 - Loss: 0.3487\n",
            "Epoch 21/25 - Loss: 0.2535\n",
            "Epoch 22/25 - Loss: 0.1925\n",
            "Epoch 23/25 - Loss: 0.1841\n",
            "Epoch 24/25 - Loss: 0.1768\n",
            "Epoch 25/25 - Loss: 0.1976\n",
            "Test Accuracy after Fine-tuning: 0.6000\n",
            "Test Precision after Fine-tuning: 0.5969\n",
            "Test Recall after Fine-tuning: 0.6000\n",
            "Test F1-Score after Fine-tuning: 0.5969\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import re\n",
        "\n",
        "# Set a random seed for reproducibility\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Check if GPU is available and set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv(r\"/content/drive/MyDrive/GujaratiTrainingData.csv\")\n",
        "\n",
        "# Preprocess the text and labels\n",
        "labels = data[\"label\"].tolist()\n",
        "texts = data[\"text\"].tolist()\n",
        "\n",
        "# Encode labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "train_texts, test_texts, train_labels, test_labels = train_test_split(texts, encoded_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Load the multilingual BERT model and tokenizer\n",
        "model_name = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(label_encoder.classes_)).to(device)\n",
        "\n",
        "# Tokenize the training and testing texts\n",
        "train_inputs = tokenizer(train_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "test_inputs = tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "train_labels_tensor = torch.tensor(train_labels)\n",
        "test_labels_tensor = torch.tensor(test_labels)\n",
        "\n",
        "# Create DataLoader for training and testing\n",
        "train_dataset = TensorDataset(train_inputs[\"input_ids\"], train_inputs[\"attention_mask\"], train_labels_tensor)\n",
        "test_dataset = TensorDataset(test_inputs[\"input_ids\"], test_inputs[\"attention_mask\"], test_labels_tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)  # No need to shuffle for testing\n",
        "\n",
        "# Define optimizer and loss function for fine-tuning\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)  # Adjust the learning rate\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "# Fine-tuning loop\n",
        "num_epochs = 25  # You can adjust the number of epochs\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids, attention_mask, batch_labels = [item.to(device) for item in batch]\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        loss = criterion(logits, batch_labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {average_loss:.4f}\")\n",
        "\n",
        "# Evaluation after fine-tuning\n",
        "model.eval()\n",
        "test_predictions = []\n",
        "test_true_labels = []\n",
        "with torch.no_grad():\n",
        "    for batch in test_dataloader:\n",
        "        input_ids, attention_mask, batch_labels = [item.to(device) for item in batch]\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        logits = outputs.logits\n",
        "        probabilities = torch.softmax(logits, dim=1)\n",
        "        predictions = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "        test_predictions.extend(predictions.cpu().numpy())\n",
        "        test_true_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "accuracy = accuracy_score(test_true_labels, test_predictions)\n",
        "precision = precision_score(test_true_labels, test_predictions, average=\"weighted\", zero_division=1)\n",
        "recall = recall_score(test_true_labels, test_predictions, average=\"weighted\", zero_division=1)\n",
        "f1 = f1_score(test_true_labels, test_predictions, average=\"weighted\", zero_division=1)\n",
        "\n",
        "print(f\"Test Accuracy after Fine-tuning: {accuracy:.4f}\")\n",
        "print(f\"Test Precision after Fine-tuning: {precision:.4f}\")\n",
        "print(f\"Test Recall after Fine-tuning: {recall:.4f}\")\n",
        "print(f\"Test F1-Score after Fine-tuning: {f1:.4f}\")\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}